"""Provides the LogArchiveReader class for efficiently reading and iterating through .npz log archives generated by
DataLogger instances.
"""

from math import ceil
from pathlib import Path
from functools import cached_property
from dataclasses import dataclass
from collections.abc import Iterator

import numpy as np
from numpy.typing import NDArray
from ataraxis_base_utilities import console, resolve_worker_count


@dataclass(frozen=True, slots=True)
class LogMessage:
    """Stores a single message extracted from a log archive.

    Notes:
        This class is yielded by the LogArchiveReader.iter_messages() method for each message in the archive. The
        structure of the payload is domain-specific and must be parsed by the consumer.
    """

    timestamp_us: np.uint64
    """The absolute UTC timestamp of when the message was logged, in microseconds since epoch."""

    payload: NDArray[np.uint8]
    """The message payload as a byte array."""


class LogArchiveReader:
    """Reads and iterates through .npz log archives generated by DataLogger instances.

    This class provides efficient access to log archive contents with support for parallel batch processing. It handles
    onset timestamp discovery, message iteration, and batch assignment for multiprocessing workflows.

    Notes:
        Each .npz archive contains messages from a single source (producer). Messages are stored with the structure:
        [source_id (1 byte)][timestamp (8 bytes)][payload (N bytes)].

        The first message with a timestamp value of 0 contains the onset timestamp as its payload. This onset timestamp
        is the UTC epoch reference used to convert elapsed microseconds to absolute timestamps.

        For multiprocessing workflows, the main process should create a reader to discover the onset timestamp and
        generate batch assignments. Worker processes can then create lightweight reader instances by passing the
        pre-discovered onset_us to skip redundant onset scanning.

    Args:
        archive_path: The path to the .npz log archive file to read.
        onset_us: The pre-discovered onset timestamp in microseconds since epoch. If provided, skips onset discovery.
            Use this in worker processes to avoid redundant scanning.

    Attributes:
        _archive_path: Stores the path to the log archive file.
        _onset_us: Stores the onset timestamp if pre-provided.
        _message_keys: Caches the list of message keys (excluding the onset message).

    Raises:
        FileNotFoundError: If the specified archive file does not exist.
    """

    # The number of bytes used to store the timestamp in each message header.
    _TIMESTAMP_BYTE_SIZE: int = 8

    # The threshold for parallel processing. Archives with fewer messages are processed sequentially.
    _PARALLEL_PROCESSING_THRESHOLD: int = 2000

    # The suffix pattern for onset message keys (underscore + 20 zeros for timestamp=0).
    _ONSET_KEY_SUFFIX: str = "_00000000000000000000"

    def __init__(self, archive_path: Path, onset_us: np.uint64 | None = None) -> None:
        # Validates the archive path.
        if not archive_path.exists():
            message = f"Unable to open log archive. The file does not exist at the expected path: {archive_path}."
            console.error(message=message, error=FileNotFoundError)

        self._archive_path: Path = archive_path
        self._onset_us: np.uint64 | None = onset_us
        self._message_keys: list[str] | None = None

    def __repr__(self) -> str:
        """Returns the string representation of the LogArchiveReader instance."""
        onset_str = str(self._onset_us) if self._onset_us is not None else "not discovered"
        return f"LogArchiveReader(archive_path={self._archive_path}, onset_us={onset_str})"

    @cached_property
    def onset_timestamp_us(self) -> np.uint64:
        """Returns the onset timestamp in microseconds since epoch.

        Notes:
            The onset timestamp is the UTC epoch reference stored in the first message with a timestamp value of 0.
            All other message timestamps are stored as elapsed microseconds relative to this onset.

            If onset_us was provided during initialization, returns that value without scanning the archive.
            Otherwise, scans the archive to discover the onset timestamp and caches the result.

        Returns:
            The onset timestamp as a numpy uint64 value representing microseconds since epoch.

        Raises:
            ValueError: If the archive does not contain a valid onset timestamp message.
        """
        # Returns the pre-provided onset timestamp if available.
        if self._onset_us is not None:
            return self._onset_us

        # Scans the archive to discover the onset timestamp.
        with np.load(self._archive_path, allow_pickle=False, mmap_mode="r") as archive:
            file_list = list(archive.files)

            for number, item in enumerate(file_list):
                entry: NDArray[np.uint8] = archive[item]

                # The timestamp occupies bytes 1-9 (8 bytes after the 1-byte source_id).
                timestamp_value = entry[1 : 1 + self._TIMESTAMP_BYTE_SIZE].view(np.uint64).item()

                # A timestamp value of 0 indicates the onset message.
                if timestamp_value == 0:
                    # The payload contains the onset timestamp as a signed 64-bit integer.
                    onset = np.uint64(entry[1 + self._TIMESTAMP_BYTE_SIZE :].view(np.int64).item())

                    # Caches the message keys excluding the onset message for later use.
                    self._message_keys = file_list[number + 1 :]

                    return onset

        # If no onset message is found, raises an error.
        error_message = (
            f"Unable to discover onset timestamp in log archive. The archive must contain a message with timestamp "
            f"value 0 storing the UTC epoch reference, but none was found in: {self._archive_path}."
        )
        console.error(message=error_message, error=ValueError)

        # This return is never reached but satisfies type checkers.
        # noinspection PyUnreachableCode
        return np.uint64(0)  # pragma: no cover

    def _get_message_keys(self) -> list[str]:
        """Returns the cached list of message keys, loading and filtering if necessary.

        Notes:
            When onset was pre-provided (skipping discovery), finds the onset key by its filename pattern (ends with
            20 zeros for timestamp=0), then slices from that index. This avoids iterating over all keys.

        Returns:
            A list of string keys for data messages (excluding the onset message).
        """
        if self._message_keys is None:
            # Triggers onset discovery which populates _message_keys via slicing.
            _ = self.onset_timestamp_us

            # If still None after onset discovery (e.g., onset was pre-provided), finds onset index and slices.
            if self._message_keys is None:
                with np.load(self._archive_path, allow_pickle=False, mmap_mode="r") as archive:
                    all_keys = list(archive.files)

                    # Finds the onset key index by filename pattern and slices from there.
                    for index, key in enumerate(all_keys):
                        if key.endswith(self._ONSET_KEY_SUFFIX):
                            self._message_keys = all_keys[index + 1 :]
                            break
                    else:
                        # No onset key found by pattern - return all keys (edge case).
                        self._message_keys = all_keys

        return self._message_keys

    @property
    def message_count(self) -> int:
        """Returns the number of data messages in the archive, excluding the onset message.

        Returns:
            The count of data messages available for iteration.
        """
        return len(self._get_message_keys())

    def get_batches(self, workers: int = -1, batch_multiplier: int = 4) -> list[list[str]]:
        """Divides message keys into batches optimized for parallel processing.

        Notes:
            Uses over-batching (creating more batches than workers) to improve load balancing when message processing
            times vary. The batch_multiplier parameter controls the degree of over-batching.

            For archives with fewer messages than the parallel processing threshold (2000), returns a single batch
            containing all message keys.

        Args:
            workers: The number of worker processes to optimize batching for. A value less than 1 uses all available
                CPU cores minus 2.
            batch_multiplier: The over-batching factor. Creates (workers * batch_multiplier) batches for better load
                distribution.

        Returns:
            A list of message key batches. Each batch is a list of string keys that can be passed to iter_messages().
        """
        keys = self._get_message_keys()
        total_messages = len(keys)

        # For small archives, returns a single batch.
        if total_messages < self._PARALLEL_PROCESSING_THRESHOLD:
            return [keys] if keys else []

        # Resolves the worker count.
        resolved_workers = resolve_worker_count(requested_workers=max(0, workers))

        # Calculates batch size using over-batching for better load distribution.
        batch_size = max(1, ceil(total_messages / (resolved_workers * batch_multiplier)))

        # Splits keys into batches and returns them.
        return [keys[i : i + batch_size] for i in range(0, total_messages, batch_size)]

    def iter_messages(self, keys: list[str] | None = None) -> Iterator[LogMessage]:
        """Iterates through messages in the archive, yielding LogMessage instances.

        Notes:
            Opens the archive with memory mapping for efficient access. The archive is kept open for the duration of
            iteration.

            If keys is provided, only iterates through the specified messages. This is useful for processing a batch
            of messages in a worker process.

        Args:
            keys: Optional list of message keys to iterate. If None, iterates through all data messages in the archive.

        Yields:
            LogMessage instances containing the absolute timestamp and payload for each message.
        """
        # Resolves the onset timestamp (triggers discovery if needed).
        onset = self.onset_timestamp_us

        # Resolves the keys to iterate.
        target_keys = keys if keys is not None else self._get_message_keys()

        # Opens the archive and iterates through messages.
        with np.load(self._archive_path, allow_pickle=False, mmap_mode="r") as archive:
            for key in target_keys:
                message: NDArray[np.uint8] = archive[key]

                # Extracts the elapsed microseconds from the message header.
                elapsed_us = message[1 : 1 + self._TIMESTAMP_BYTE_SIZE].view(np.uint64).item()

                # Calculates the absolute timestamp.
                absolute_timestamp = onset + elapsed_us

                # Extracts the payload (everything after the header).
                payload = message[1 + self._TIMESTAMP_BYTE_SIZE :].copy()

                yield LogMessage(timestamp_us=np.uint64(absolute_timestamp), payload=payload)

    def read_all_messages(self) -> tuple[NDArray[np.uint64], list[NDArray[np.uint8]]]:
        """Reads all messages from the archive and returns them as arrays.

        Notes:
            This method loads all messages into memory at once. For very large archives, consider using iter_messages()
            with batching instead.

        Returns:
            A tuple of two elements. The first element is a numpy array of absolute timestamps in microseconds. The
            second element is a list of payload arrays, one for each message.
        """
        timestamps: list[np.uint64] = []
        payloads: list[NDArray[np.uint8]] = []

        for message in self.iter_messages():
            timestamps.append(message.timestamp_us)
            payloads.append(message.payload)

        return np.array(timestamps, dtype=np.uint64), payloads
